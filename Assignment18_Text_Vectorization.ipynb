{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb965da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...   \n",
      "1   ham                      Ok lar... Joking wif u oni...   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3   ham  U dun say so early hor... U c already then say...   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                    final_clean_text  \n",
      "0  go until jurong point, crazy.. available only ...  \n",
      "1                      ok lar... joking wif u oni...  \n",
      "2  free entry in 2 a wkly comp to win fa cup fina...  \n",
      "3  u dun say so early hor... u c already then say...  \n",
      "4  nah i don't think he goes to usf, he lives aro...  \n",
      "Dataset size: (5572, 3)\n"
     ]
    }
   ],
   "source": [
    "# ASSIGNMENT 18 — TEXT VECTORIZATION TECHNIQUES\n",
    "# NLP Feature Extraction Only (NO ML MODELS)\n",
    "\n",
    "\n",
    "# PART 0 — IMPORT LIBRARIES\n",
    "\n",
    "\n",
    "# Allowed libraries only\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Load Dataset\n",
    "# ------------------------------\n",
    "\n",
    "# Download spam.csv from Kaggle dataset\n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "\n",
    "# Keep required columns only\n",
    "df = df[['v1','v2']]\n",
    "df.columns = ['label','text']\n",
    "\n",
    "# Use cleaned column (simulate previous assignment output)\n",
    "df['final_clean_text'] = df['text'].str.lower()\n",
    "\n",
    "print(df.head())\n",
    "print(\"Dataset size:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " [\"08452810075over18's\", '2', '2005.', '21st', '87121', 'a', 'already', 'amore', 'apply', 'around', 'available', 'buffet...', 'bugis', 'c', 'cine', 'comp', 'crazy..', 'cup', \"don't\", 'dun', 'e', 'early', 'entry', 'fa', 'final', 'free', 'go', 'goes', 'got', 'great', 'he', 'here', 'hor...', 'i', 'in', 'joking', 'jurong', 'la', 'lar...', 'lives', 'may', 'n', 'nah', 'ok', 'oni...', 'only', 'point,', 'question(std', \"rate)t&c's\", 'receive', 'say', 'say...', 'so', 'text', 'then', 'there', 'think', 'though', 'tkts', 'to', 'txt', 'u', 'until', 'usf,', 'wat...', 'wif', 'win', 'wkly', 'world']\n",
      "\n",
      "One Hot Encoded Matrix:\n",
      "   08452810075over18's  2  2005.  21st  87121  a  already  amore  apply  \\\n",
      "0                    0  0      0     0      0  0        0      1      0   \n",
      "1                    0  0      0     0      0  0        0      0      0   \n",
      "2                    1  1      1     1      1  1        0      0      1   \n",
      "3                    0  0      0     0      0  0        1      0      0   \n",
      "4                    0  0      0     0      0  0        0      0      0   \n",
      "\n",
      "   around  ...  to  txt  u  until  usf,  wat...  wif  win  wkly  world  \n",
      "0       0  ...   0    0  0      1     0       1    0    0     0      1  \n",
      "1       0  ...   0    0  1      0     0       0    1    0     0      0  \n",
      "2       0  ...   1    1  0      0     0       0    0    1     1      0  \n",
      "3       0  ...   0    0  1      0     0       0    0    0     0      0  \n",
      "4       1  ...   1    0  0      0     1       0    0    0     0      0  \n",
      "\n",
      "[5 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 1 — TASK 1\n",
    "# Manual One-Hot Encoding\n",
    "\n",
    "\n",
    "# Step 1: Select 5 sentences\n",
    "sentences = df['final_clean_text'].iloc[:5].tolist()\n",
    "\n",
    "# Step 2: Build vocabulary manually\n",
    "vocab = sorted(set(\" \".join(sentences).split()))\n",
    "print(\"Vocabulary:\\n\", vocab)\n",
    "\n",
    "# Step 3: Create one-hot matrix manually\n",
    "one_hot_matrix = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    vector = [1 if word in words else 0 for word in vocab]\n",
    "    one_hot_matrix.append(vector)\n",
    "\n",
    "# Step 4: Display\n",
    "one_hot_df = pd.DataFrame(one_hot_matrix, columns=vocab)\n",
    "print(\"\\nOne Hot Encoded Matrix:\")\n",
    "print(one_hot_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fedd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'go': 22, 'until': 55, 'jurong': 31, 'point': 40, 'crazy': 13, 'available': 8, 'only': 39, 'in': 29, 'bugis': 10, 'great': 25, 'world': 61, 'la': 32, 'buffet': 9, 'cine': 11, 'there': 49, 'got': 24, 'amore': 5, 'wat': 57, 'ok': 37, 'lar': 33, 'joking': 30, 'wif': 58, 'oni': 38, 'free': 21, 'entry': 18, 'wkly': 60, 'comp': 12, 'to': 53, 'win': 59, 'fa': 19, 'cup': 14, 'final': 20, 'tkts': 52, '21st': 2, 'may': 35, '2005': 1, 'text': 47, '87121': 3, 'receive': 43, 'question': 41, 'std': 46, 'txt': 54, 'rate': 42, 'apply': 6, '08452810075over18': 0, 'dun': 16, 'say': 44, 'so': 45, 'early': 17, 'hor': 28, 'already': 4, 'then': 48, 'nah': 36, 'don': 15, 'think': 50, 'he': 26, 'goes': 23, 'usf': 56, 'lives': 34, 'around': 7, 'here': 27, 'though': 51}\n",
      "\n",
      "Encoded Matrix:\n",
      "   08452810075over18  2005  21st  87121  already  amore  apply  around  \\\n",
      "0                  0     0     0      0        0      1      0       0   \n",
      "1                  0     0     0      0        0      0      0       0   \n",
      "2                  1     1     1      1        0      0      1       0   \n",
      "3                  0     0     0      0        1      0      0       0   \n",
      "4                  0     0     0      0        0      0      0       1   \n",
      "\n",
      "   available  buffet  ...  tkts  to  txt  until  usf  wat  wif  win  wkly  \\\n",
      "0          1       1  ...     0   0    0      1    0    1    0    0     0   \n",
      "1          0       0  ...     0   0    0      0    0    0    1    0     0   \n",
      "2          0       0  ...     1   1    1      0    0    0    0    1     1   \n",
      "3          0       0  ...     0   0    0      0    0    0    0    0     0   \n",
      "4          0       0  ...     0   1    0      0    1    0    0    0     0   \n",
      "\n",
      "   world  \n",
      "0      1  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "\n",
      "[5 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 1 — TASK 2\n",
    "# One-Hot Encoding using CountVectorizer\n",
    "\n",
    "\n",
    "# Step 1: Initialize vectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Step 2: Fit & Transform\n",
    "encoded = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Step 3: Display vocabulary\n",
    "print(\"Vocabulary:\\n\", vectorizer.vocabulary_)\n",
    "\n",
    "# Step 4: Display matrix\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\"\\nEncoded Matrix:\")\n",
    "print(encoded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32af20d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8672\n",
      "\n",
      "Sample BoW vectors:\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 2 — TASK 3\n",
    "# Bag of Words\n",
    "\n",
    "\n",
    "# Step 1: Initialize BoW\n",
    "bow = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit entire dataset\n",
    "bow_matrix = bow.fit_transform(df['final_clean_text'])\n",
    "\n",
    "# Step 3: Vocabulary size\n",
    "print(\"Vocabulary Size:\", len(bow.vocabulary_))\n",
    "\n",
    "# Step 4: Sample feature vectors\n",
    "print(\"\\nSample BoW vectors:\")\n",
    "print(bow_matrix[:5].toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea469d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 frequent words:\n",
      "     word  count\n",
      "7756   to   2242\n",
      "8609  you   2240\n",
      "7627  the   1328\n",
      "1084  and    979\n",
      "4087   in    898\n",
      "4206   is    890\n",
      "4939   me    802\n",
      "5223   my    762\n",
      "4218   it    744\n",
      "3308  for    704\n",
      "\n",
      "Least frequent words:\n",
      "             word  count\n",
      "3591         gosh      1\n",
      "33    07753741225      1\n",
      "3592         goss      1\n",
      "4846  maintaining      1\n",
      "4842       mailed      1\n",
      "37         077xxx      1\n",
      "38            078      1\n",
      "39    07801543489      1\n",
      "40          07808      1\n",
      "41    07808247860      1\n",
      "\n",
      "BoW captures frequency by counting occurrences of words across documents.\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# PART 2 — TASK 4\n",
    "# Word Frequency Analysis\n",
    "############################################################\n",
    "\n",
    "# Step 1: Total frequency\n",
    "word_counts = np.array(bow_matrix.sum(axis=0)).flatten()\n",
    "\n",
    "words = bow.get_feature_names_out()\n",
    "freq_df = pd.DataFrame({'word': words, 'count': word_counts})\n",
    "\n",
    "# Step 2: Top frequent words\n",
    "print(\"\\nTop 10 frequent words:\")\n",
    "print(freq_df.sort_values(by='count', ascending=False).head(10))\n",
    "\n",
    "# Step 3: Least frequent words\n",
    "print(\"\\nLeast frequent words:\")\n",
    "print(freq_df.sort_values(by='count').head(10))\n",
    "\n",
    "# Explanation:\n",
    "print(\"\\nBoW captures frequency by counting occurrences of words across documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cf580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram vocab size: 8672\n",
      "Bigram vocab size: 41654\n",
      "Trigram vocab size: 54238\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 3 — TASK 5\n",
    "# N-Gram Comparison\n",
    "\n",
    "\n",
    "# Unigrams\n",
    "uni = CountVectorizer(ngram_range=(1,1))\n",
    "uni_mat = uni.fit_transform(df['final_clean_text'])\n",
    "\n",
    "# Bigrams\n",
    "bi = CountVectorizer(ngram_range=(2,2))\n",
    "bi_mat = bi.fit_transform(df['final_clean_text'])\n",
    "\n",
    "# Trigrams\n",
    "tri = CountVectorizer(ngram_range=(3,3))\n",
    "tri_mat = tri.fit_transform(df['final_clean_text'])\n",
    "\n",
    "print(\"Unigram vocab size:\", len(uni.vocabulary_))\n",
    "print(\"Bigram vocab size:\", len(bi.vocabulary_))\n",
    "print(\"Trigram vocab size:\", len(tri.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb62c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined vocab size: 50326\n",
      "\n",
      "Context improves because phrases like 'free offer' are captured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 3 — TASK 6\n",
    "# Combined N-grams\n",
    "\n",
    "\n",
    "combined = CountVectorizer(ngram_range=(1,2))\n",
    "combined_matrix = combined.fit_transform(df['final_clean_text'])\n",
    "\n",
    "print(\"Combined vocab size:\", len(combined.vocabulary_))\n",
    "\n",
    "print(\"\\nContext improves because phrases like 'free offer' are captured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370766f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8672\n",
      "TF-IDF Matrix Shape: (5572, 8672)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 4 — TASK 7\n",
    "# TF-IDF Vectorization\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf.fit_transform(df['final_clean_text'])\n",
    "\n",
    "print(\"Vocabulary Size:\", len(tfidf.vocabulary_))\n",
    "print(\"TF-IDF Matrix Shape:\", tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81710896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "High TF-IDF words:\n",
      "      word  tfidf_score\n",
      "8609   you     0.044145\n",
      "7756    to     0.036946\n",
      "7627   the     0.026384\n",
      "4087    in     0.021946\n",
      "4939    me     0.021159\n",
      "1084   and     0.020217\n",
      "4206    is     0.019703\n",
      "4218    it     0.018706\n",
      "5223    my     0.018602\n",
      "1813  call     0.017207\n",
      "\n",
      "Low TF-IDF words:\n",
      "              word  tfidf_score\n",
      "4850       makiing     0.000013\n",
      "6002       praises     0.000013\n",
      "7075       sorrows     0.000013\n",
      "6115        proove     0.000013\n",
      "6597        sambar     0.000013\n",
      "1279    attraction     0.000013\n",
      "2566    determined     0.000014\n",
      "7331       stylist     0.000014\n",
      "6146       pudunga     0.000014\n",
      "6138  psychologist     0.000014\n",
      "\n",
      "TF-IDF down-weights very common words appearing in many documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 4 — TASK 8\n",
    "# BoW vs TF-IDF Comparison\n",
    "\n",
    "\n",
    "tfidf_scores = np.array(tfidf_matrix.mean(axis=0)).flatten()\n",
    "\n",
    "tfidf_df = pd.DataFrame({\n",
    "    'word': tfidf.get_feature_names_out(),\n",
    "    'tfidf_score': tfidf_scores\n",
    "})\n",
    "\n",
    "print(\"\\nHigh TF-IDF words:\")\n",
    "print(tfidf_df.sort_values(by='tfidf_score', ascending=False).head(10))\n",
    "\n",
    "print(\"\\nLow TF-IDF words:\")\n",
    "print(tfidf_df.sort_values(by='tfidf_score').head(10))\n",
    "\n",
    "print(\"\\nTF-IDF down-weights very common words appearing in many documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f02464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (max_features=1000): 1000\n",
      "Vocab size (min_df=5): 1812\n",
      "Vocab size (max_df=0.8): 8672\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# PART 5 — TASK 9\n",
    "# Parameter Exploration\n",
    "\n",
    "vec1 = CountVectorizer(max_features=1000)\n",
    "vec2 = CountVectorizer(min_df=5)\n",
    "vec3 = CountVectorizer(max_df=0.8)\n",
    "\n",
    "vec1.fit(df['final_clean_text'])\n",
    "vec2.fit(df['final_clean_text'])\n",
    "vec3.fit(df['final_clean_text'])\n",
    "\n",
    "print(\"Vocab size (max_features=1000):\", len(vec1.vocabulary_))\n",
    "print(\"Vocab size (min_df=5):\", len(vec2.vocabulary_))\n",
    "print(\"Vocab size (max_df=0.8):\", len(vec3.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4192d9c",
   "metadata": {},
   "source": [
    "### **1. Difference between One-Hot Encoding and Bag of Words (BoW)**\n",
    "\n",
    "* **One-Hot Encoding** represents whether a word is present or absent in a sentence (0 or 1).\n",
    "* **Bag of Words (BoW)** represents how many times each word appears (frequency count).\n",
    "* One-Hot ignores repetition, while BoW captures word importance through frequency.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Why N-grams increase dimensionality**\n",
    "\n",
    "* N-grams create features using combinations of words (e.g., bigrams, trigrams).\n",
    "* Each unique word combination becomes a new feature.\n",
    "* As combinations grow, the vocabulary size increases rapidly, leading to higher dimensional vectors.\n",
    "\n",
    "\n",
    "\n",
    "### **3. When to prefer TF-IDF over BoW**\n",
    "\n",
    "* TF-IDF is preferred when common words (like *the*, *is*, *and*) should have less importance.\n",
    "* It highlights words that are unique or meaningful in specific documents.\n",
    "* Useful in search, document similarity, and text ranking tasks.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Limitations of count-based vectorization methods**\n",
    "\n",
    "* They do not understand semantic meaning or context.\n",
    "* Produce very sparse and high-dimensional matrices.\n",
    "* Words with similar meanings are treated as completely different features.\n",
    "* Cannot capture word order effectively (especially BoW).\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
